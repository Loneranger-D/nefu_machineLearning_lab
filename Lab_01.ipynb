{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3524ed48-839e-4786-a9c1-c4031969d10d",
   "metadata": {},
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85943e6-5b4a-44fd-b0c7-3ca4b0c67690",
   "metadata": {},
   "source": [
    "## 1. Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dde0d5-0196-4e96-85b9-ca8074ea5ca8",
   "metadata": {},
   "source": [
    "### (a) Explanation of your implementation and experimental setup for task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191879eb-6df2-44e7-8a64-a7867a8ed949",
   "metadata": {},
   "source": [
    "First we observe the data and find that ID and name are less helpful for classification, latitude, longitude and mean_checkin_time are numerical values that may have an impact on the results, and review is text, which requires some processing before discussion.\n",
    "After removing unnecessary features, some representations of category are replaced by numerical values.\n",
    "The text is vectorized for word frequency using the CountVectorizer class, to find the frequency of each word.\n",
    "Since the vocabulary size of the training set is different from that of the test set, the number of features is therefore different. Since the words that are not in the test set do not help the calculation, we choose to take the intersection of the two sets before performing the calculation.\n",
    "The obtained matrix is merged into the other three features, and the data is normalized for Bayesian processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9b87ce-85e5-4946-9871-06248a2e5a3c",
   "metadata": {},
   "source": [
    "### (b) Summary and discussion of experimental results for task 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83110297-eaf7-40d6-9ca0-6e5444b18148",
   "metadata": {},
   "source": [
    "After calculation, we obtained a accuracy of model about 0.74, and it can be determined that this model can discriminate category, but there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f250d2-688e-460a-84fb-fdc8eb7e81b7",
   "metadata": {},
   "source": [
    "### (c) Attached source codes for task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e899665-c4dd-4f45-bfa5-7b59f1841be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa013a8b-9706-4c49-8480-00a3b1566474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>mean_checkin_time</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3007</td>\n",
       "      <td>The New Orleans Vampire Cafe</td>\n",
       "      <td>29.959033</td>\n",
       "      <td>-90.064036</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Amazing service. Cool vibe. It's not spooky or...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1829</td>\n",
       "      <td>Ted's Frostop</td>\n",
       "      <td>29.947026</td>\n",
       "      <td>-90.113604</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Breakfast here is great and there's never a hu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>298</td>\n",
       "      <td>The Will &amp; The Way</td>\n",
       "      <td>29.957573</td>\n",
       "      <td>-90.065827</td>\n",
       "      <td>9.5</td>\n",
       "      <td>So glad that we stumbled in here! The cheesebu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1245</td>\n",
       "      <td>Public Belt</td>\n",
       "      <td>29.946393</td>\n",
       "      <td>-90.063729</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AMAZING! Try this place out.  Great for some g...</td>\n",
       "      <td>Nightlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2902</td>\n",
       "      <td>Phillys Cafe</td>\n",
       "      <td>29.941818</td>\n",
       "      <td>-90.094797</td>\n",
       "      <td>18.0</td>\n",
       "      <td>WooHoo, best philly cheese staks I have had in...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                          name   latitude  longitude  \\\n",
       "0  3007  The New Orleans Vampire Cafe  29.959033 -90.064036   \n",
       "1  1829                 Ted's Frostop  29.947026 -90.113604   \n",
       "2   298            The Will & The Way  29.957573 -90.065827   \n",
       "3  1245                   Public Belt  29.946393 -90.063729   \n",
       "4  2902                  Phillys Cafe  29.941818 -90.094797   \n",
       "\n",
       "   mean_checkin_time                                             review  \\\n",
       "0               17.0  Amazing service. Cool vibe. It's not spooky or...   \n",
       "1               17.0  Breakfast here is great and there's never a hu...   \n",
       "2                9.5  So glad that we stumbled in here! The cheesebu...   \n",
       "3                3.0  AMAZING! Try this place out.  Great for some g...   \n",
       "4               18.0  WooHoo, best philly cheese staks I have had in...   \n",
       "\n",
       "      category  \n",
       "0  Restaurants  \n",
       "1  Restaurants  \n",
       "2  Restaurants  \n",
       "3    Nightlife  \n",
       "4  Restaurants  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the data\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "train_data = pd.read_csv(PATH_TRAIN,  header=0)\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "test_data = pd.read_csv(PATH_TEST, header=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda66b64-8e81-4772-ab50-ea9ac8e2d81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>mean_checkin_time</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.959033</td>\n",
       "      <td>-90.064036</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Amazing service. Cool vibe. It's not spooky or...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.947026</td>\n",
       "      <td>-90.113604</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Breakfast here is great and there's never a hu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.957573</td>\n",
       "      <td>-90.065827</td>\n",
       "      <td>9.5</td>\n",
       "      <td>So glad that we stumbled in here! The cheesebu...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.946393</td>\n",
       "      <td>-90.063729</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AMAZING! Try this place out.  Great for some g...</td>\n",
       "      <td>Nightlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.941818</td>\n",
       "      <td>-90.094797</td>\n",
       "      <td>18.0</td>\n",
       "      <td>WooHoo, best philly cheese staks I have had in...</td>\n",
       "      <td>Restaurants</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude  mean_checkin_time  \\\n",
       "0  29.959033 -90.064036               17.0   \n",
       "1  29.947026 -90.113604               17.0   \n",
       "2  29.957573 -90.065827                9.5   \n",
       "3  29.946393 -90.063729                3.0   \n",
       "4  29.941818 -90.094797               18.0   \n",
       "\n",
       "                                              review     category  \n",
       "0  Amazing service. Cool vibe. It's not spooky or...  Restaurants  \n",
       "1  Breakfast here is great and there's never a hu...  Restaurants  \n",
       "2  So glad that we stumbled in here! The cheesebu...  Restaurants  \n",
       "3  AMAZING! Try this place out.  Great for some g...    Nightlife  \n",
       "4  WooHoo, best philly cheese staks I have had in...  Restaurants  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for valuable features\n",
    "train_data = train_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "test_data = test_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec759f8-16ef-404c-b05f-8269189a2b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>mean_checkin_time</th>\n",
       "      <th>review</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.959033</td>\n",
       "      <td>-90.064036</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Amazing service. Cool vibe. It's not spooky or...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.947026</td>\n",
       "      <td>-90.113604</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Breakfast here is great and there's never a hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.957573</td>\n",
       "      <td>-90.065827</td>\n",
       "      <td>9.5</td>\n",
       "      <td>So glad that we stumbled in here! The cheesebu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.946393</td>\n",
       "      <td>-90.063729</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AMAZING! Try this place out.  Great for some g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.941818</td>\n",
       "      <td>-90.094797</td>\n",
       "      <td>18.0</td>\n",
       "      <td>WooHoo, best philly cheese staks I have had in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude  mean_checkin_time  \\\n",
       "0  29.959033 -90.064036               17.0   \n",
       "1  29.947026 -90.113604               17.0   \n",
       "2  29.957573 -90.065827                9.5   \n",
       "3  29.946393 -90.063729                3.0   \n",
       "4  29.941818 -90.094797               18.0   \n",
       "\n",
       "                                              review  category  \n",
       "0  Amazing service. Cool vibe. It's not spooky or...         0  \n",
       "1  Breakfast here is great and there's never a hu...         0  \n",
       "2  So glad that we stumbled in here! The cheesebu...         0  \n",
       "3  AMAZING! Try this place out.  Great for some g...         1  \n",
       "4  WooHoo, best philly cheese staks I have had in...         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting category labels to numbers\n",
    "classes = train_data['category'].unique()\n",
    "classes_types = CategoricalDtype(categories=classes)\n",
    "train_data['category'] = train_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "train_data_review = train_data['review']\n",
    "test_data_review = test_data['review']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb57b595-b094-45cd-9310-453742dc7c32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is :\n",
      "0.7359307359307359\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def type1(train_data,train_data_review,test_data,test_data_review):\n",
    "    def process_review_CountVectorizer(data_review):\n",
    "        '''\n",
    "        Find the word frequency\n",
    "        '''\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(data_review)\n",
    "        T = vectorizer.get_feature_names()\n",
    "        X = pd.DataFrame(X.toarray(),columns=T)\n",
    "        X.index.name ='merge_index_unique'\n",
    "        return X,T\n",
    "    \n",
    "    train_y = train_data['category'].to_numpy()\n",
    "    test_y = test_data['category'].to_numpy()\n",
    "    \n",
    "    # Incorporate word frequencies into other features\n",
    "    new1,T1 = process_review_CountVectorizer(train_data_review)\n",
    "    new2,T2 = process_review_CountVectorizer(test_data_review)\n",
    "    # 求训练集和测试集词汇量的交集，因为不在测试集的词汇训练了也没有用\n",
    "    T = set(T1)&set(T2)\n",
    "    T = list(T)\n",
    "    # 合并训练集和测试集的特征\n",
    "    new1 = new1[T]\n",
    "    train_data.index.name ='merge_index_unique'\n",
    "    train_data_new = pd.merge(train_data.reset_index(),new1.reset_index(),on='merge_index_unique')\n",
    "    \n",
    "    new2 = new2[T]\n",
    "    test_data.index.name ='merge_index_unique'\n",
    "    test_data_new = pd.merge(test_data.reset_index(),new2.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    train_x = train_data_new.drop(['category','review_x'], axis=1)\n",
    "    test_x = test_data_new.drop(['category','review_x'], axis=1)\n",
    "    \n",
    "    # Normalize\n",
    "    train_x = (train_x-train_x.min())/(train_x.max()-train_x.min())\n",
    "    test_x = (test_x-test_x.min())/(test_x.max()-test_x.min())\n",
    "    \n",
    "    train_x = train_x.to_numpy()\n",
    "    test_x = test_x.to_numpy()\n",
    "    \n",
    "    # Train and test the model\n",
    "    nb_new = GaussianNB()\n",
    "    nb_new.fit(train_x, train_y)\n",
    "    print(\"The score is :\")\n",
    "    print(nb_new.score(test_x,test_y))\n",
    "    print()\n",
    "type1(train_data,train_data_review,test_data,test_data_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d720c-fbba-4627-a42b-9ca54e7f6e5c",
   "metadata": {},
   "source": [
    "## 2. Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff45a5-42eb-4b0c-b4b9-4e1b9b7b8a8e",
   "metadata": {},
   "source": [
    "### (a) Explanation of your implementation and experimental setup for task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c1f28-1a2d-4c50-aade-e1b96eb0ab5c",
   "metadata": {},
   "source": [
    "Because of the sparse word frequency distribution, polynomial Bayesian is chosen for processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c611a0c3-c4ac-46d3-a8ef-4cd0fa381137",
   "metadata": {},
   "source": [
    "### (b) Summary and discussion of experimental results for task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cd82f-a3a7-45df-a9d8-4350f93758f4",
   "metadata": {},
   "source": [
    "After calculation, we obtained a accuracy of about 0.858, which has some accuracy improvement, but there is still room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d570e8-3e15-4996-ac93-bd33cc6bde08",
   "metadata": {},
   "source": [
    "### (c) Attached source codes for task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc14796d-7b01-4665-8828-750ccae042f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is :\n",
      "0.862914862914863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "train_data = pd.read_csv(PATH_TRAIN,  header=0)\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "test_data = pd.read_csv(PATH_TEST, header=0)\n",
    "train_data = train_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "test_data = test_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "train_data.head()\n",
    "classes = train_data['category'].unique()\n",
    "classes_types = CategoricalDtype(categories=classes)\n",
    "train_data['category'] = train_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "train_data_review = train_data['review']\n",
    "test_data_review = test_data['review']\n",
    "\n",
    "def type1(train_data,train_data_review,test_data,test_data_review):\n",
    "    def process_review_CountVectorizer(data_review):\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(data_review)\n",
    "        T = vectorizer.get_feature_names()\n",
    "        X = pd.DataFrame(X.toarray(),columns=T)\n",
    "        X.index.name ='merge_index_unique'\n",
    "        return X,T\n",
    "    \n",
    "    train_y = train_data['category'].to_numpy()\n",
    "    test_y = test_data['category'].to_numpy()\n",
    "    \n",
    "    # Incorporate word frequencies into other features\n",
    "    new1,T1 = process_review_CountVectorizer(train_data_review)\n",
    "    new2,T2 = process_review_CountVectorizer(test_data_review)\n",
    "    # 求训练集和测试集词汇量的交集，因为不在测试集的词汇训练了也没有用\n",
    "    T = set(T1)&set(T2)\n",
    "    T = list(T)\n",
    "    # 合并训练集和测试集的特征\n",
    "    new1 = new1[T]\n",
    "    train_data.index.name ='merge_index_unique'\n",
    "    train_data = train_data.drop(['category','review'], axis=1)\n",
    "    train_data_new = pd.merge(train_data.reset_index(),new1.reset_index(),on='merge_index_unique')\n",
    "    \n",
    "    new2 = new2[T]\n",
    "    test_data.index.name ='merge_index_unique'\n",
    "    test_data = test_data.drop(['category','review'], axis=1)\n",
    "    test_data_new = pd.merge(test_data.reset_index(),new2.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    train_x = train_data_new\n",
    "    test_x = test_data_new\n",
    "    \n",
    "    # Normalize\n",
    "    train_x = (train_x-train_x.min())/(train_x.max()-train_x.min())\n",
    "    test_x = (test_x-test_x.min())/(test_x.max()-test_x.min())\n",
    "    \n",
    "    train_x = train_x.to_numpy()\n",
    "    test_x = test_x.to_numpy()\n",
    "    \n",
    "    # Train and test the model by Multinomial Naive Bayes\n",
    "    nb_new = MultinomialNB()\n",
    "    nb_new.fit(train_x, train_y)\n",
    "    print(\"The score is :\")\n",
    "    print(nb_new.score(test_x,test_y))\n",
    "    print()\n",
    "type1(train_data,train_data_review,test_data,test_data_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275fde2-3c87-4e2b-9624-fc9a38db1de5",
   "metadata": {},
   "source": [
    "## 3. Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3fbcd-19bd-4d9a-afe0-93821af6e5a1",
   "metadata": {},
   "source": [
    "### (a) Explanation of your implementation and experimental setup for task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380e464-0c3b-4c93-80a8-3d0429895b98",
   "metadata": {},
   "source": [
    "The polynomial distribution deals with discrete variables, while the other three features are continuous variables except for review. So we try to compute the text word frequency with Bayesian first, and then merge the obtained probability value with other features to continue the computation; and the data of the second computation is not discrete, so we choose to use Gaussian Bayesian for the computation. We also use the char_wb analyzer to reduce the impact of word errors when processing the text word frequencies. We choose to divide the training set into training set and verification set to preliminarily test which parameter can achieve better results, so we finally choose (5,6) as the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698bee48-1c04-4230-a9ad-1daf121862a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (b) Summary and discussion of experimental results for task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a40f7-c107-427d-8d27-71837aeab1c8",
   "metadata": {},
   "source": [
    "After the calculation, we obtained a correct rate of about 0.98, which has huge accuracy improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15e850-c578-458e-a6e8-f46986c71121",
   "metadata": {},
   "source": [
    "### (c) Attached source codes for task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c00201-c26f-4872-9706-d51e091aead9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1  The score is :  0.33681765389082463\n",
      "1 2  The score is :  0.627177700348432\n",
      "1 3  The score is :  0.5609756097560976\n",
      "2 2  The score is :  0.7770034843205574\n",
      "2 3  The score is :  0.7537746806039489\n",
      "2 4  The score is :  0.7630662020905923\n",
      "3 3  The score is :  0.8408826945412311\n",
      "3 4  The score is :  0.8292682926829268\n",
      "3 5  The score is :  0.8315911730545877\n",
      "4 4  The score is :  0.8908246225319396\n",
      "4 5  The score is :  0.8873403019744484\n",
      "4 6  The score is :  0.8850174216027874\n",
      "5 5  The score is :  0.9036004645760743\n",
      "5 6  The score is :  0.8966318234610917\n",
      "5 7  The score is :  0.8954703832752613\n",
      "6 6  The score is :  0.9070847851335656\n",
      "6 7  The score is :  0.908246225319396\n",
      "6 8  The score is :  0.8931475029036005\n",
      "7 7  The score is :  0.89198606271777\n",
      "7 8  The score is :  0.8850174216027874\n",
      "7 9  The score is :  0.8722415795586528\n"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "train_data = pd.read_csv(PATH_TRAIN,  header=0)\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "test_data = pd.read_csv(PATH_TEST, header=0)\n",
    "train_data = train_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "test_data = test_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "\n",
    "#从训练集中划分验证集\n",
    "train_data:pd.DataFrame = train_data.sample(frac=1.0)                     #将数据打乱\n",
    "rows, cols = train_data.shape\n",
    "split_index_1 = int(rows * 0.3)\n",
    "\n",
    "#数据分割\n",
    "test_data:pd.DataFrame = train_data.iloc[0: split_index_1, :].reset_index()\n",
    "train_data:pd.DataFrame = train_data.iloc[split_index_1: rows, :].reset_index()\n",
    "\n",
    "train_data.head()\n",
    "classes = train_data['category'].unique()\n",
    "classes_types = CategoricalDtype(categories=classes)\n",
    "train_data['category'] = train_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "train_data_review = train_data['review']\n",
    "test_data_review = test_data['review']\n",
    "\n",
    "def type3(train_data,train_data_review,test_data,test_data_review, ngram_range):\n",
    "    def process_review_CountVectorizer(data_review, ngram_range):\n",
    "        '''\n",
    "        Training with char_wb analyzer and 6 gram.\n",
    "        '''\n",
    "        vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=ngram_range)\n",
    "        X = vectorizer.fit_transform(data_review)\n",
    "        T = vectorizer.get_feature_names()\n",
    "        X = pd.DataFrame(X.toarray(),columns=T)\n",
    "        X.index.name ='merge_index_unique'\n",
    "        return X,T\n",
    "\n",
    "    train_y = train_data['category'].to_numpy()\n",
    "    test_y = test_data['category'].to_numpy()\n",
    "    \n",
    "    new1,T1 = process_review_CountVectorizer(train_data_review, ngram_range)\n",
    "    new2,T2 = process_review_CountVectorizer(test_data_review, ngram_range)\n",
    "    T = set(T1)&set(T2)\n",
    "    T = list(T)\n",
    "    new1 = new1[T]\n",
    "    # Train the model with word frequency by Multinomial Naive Bayes, and get the Probability for each reasult as features\n",
    "    nb1 = MultinomialNB()\n",
    "    nb1.fit(new1, train_y)\n",
    "    a = nb1.predict_proba(new1)\n",
    "    dic ={\n",
    "    '0': a[:,0],\n",
    "    '1': a[:,1],\n",
    "    '2': a[:,2]\n",
    "    }\n",
    "    new = pd.DataFrame(dic)\n",
    "    new.index.name ='merge_index_unique'\n",
    "    train_data.index.name ='merge_index_unique'   \n",
    "    train_data_new = pd.merge(train_data.reset_index(),new.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    new2 = new2[T]\n",
    "    nb2 = MultinomialNB()\n",
    "    nb2.fit(new2, test_y)\n",
    "    a = nb2.predict_proba(new2)\n",
    "    dic ={\n",
    "    '0': a[:,0],\n",
    "    '1': a[:,1],\n",
    "    '2': a[:,2]\n",
    "    }\n",
    "    new = pd.DataFrame(dic)\n",
    "    new.index.name ='merge_index_unique'\n",
    "    test_data.index.name ='merge_index_unique'\n",
    "    test_data_new = pd.merge(test_data.reset_index(),new.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    train_x = train_data_new.drop(['category','review'], axis=1)\n",
    "    test_x = test_data_new.drop(['category','review'], axis=1)\n",
    "\n",
    "    train_x = (train_x-train_x.min())/(train_x.max()-train_x.min())\n",
    "    test_x = (test_x-test_x.min())/(test_x.max()-test_x.min())\n",
    "\n",
    "    train_x = train_x.to_numpy()\n",
    "    test_x = test_x.to_numpy()\n",
    "\n",
    "    nb_new = GaussianNB()\n",
    "    nb_new.fit(train_x, train_y)\n",
    "    print(\"The score is :\",end=\"  \")\n",
    "    print(nb_new.score(test_x,test_y))\n",
    "\n",
    "\n",
    "for i in range(1,8):\n",
    "    for j in range(i,i+3):\n",
    "        print(i,j,end=\"  \")\n",
    "        type3(train_data,train_data_review,test_data,test_data_review,(i,j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74ad6a8-99dc-4817-8fad-987c3de98bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score is :\n",
      "0.9812409812409812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the data\n",
    "PATH_ROOT = os.getcwd()\n",
    "PATH_TRAIN = os.path.join(PATH_ROOT, 'train.csv')\n",
    "train_data = pd.read_csv(PATH_TRAIN,  header=0)\n",
    "PATH_TEST = os.path.join(PATH_ROOT, 'test.csv')\n",
    "test_data = pd.read_csv(PATH_TEST, header=0)\n",
    "train_data = train_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "test_data = test_data[['latitude','longitude','mean_checkin_time','review','category']]\n",
    "train_data.head()\n",
    "classes = train_data['category'].unique()\n",
    "classes_types = CategoricalDtype(categories=classes)\n",
    "train_data['category'] = train_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "test_data['category'] = test_data['category'].astype(classes_types).cat.codes.astype('long')\n",
    "train_data_review = train_data['review']\n",
    "test_data_review = test_data['review']\n",
    "\n",
    "def type4(train_data,train_data_review,test_data,test_data_review, ngram_range):\n",
    "    def process_review_CountVectorizer(data_review, ngram_range):\n",
    "        '''\n",
    "        Training with char_wb analyzer and gram.\n",
    "        '''\n",
    "        vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=ngram_range)\n",
    "        X = vectorizer.fit_transform(data_review)\n",
    "        T = vectorizer.get_feature_names()\n",
    "        X = pd.DataFrame(X.toarray(),columns=T)\n",
    "        X.index.name ='merge_index_unique'\n",
    "        return X,T\n",
    "\n",
    "    train_y = train_data['category'].to_numpy()\n",
    "    test_y = test_data['category'].to_numpy()\n",
    "    # 分别对训练集和测试集进行词频处理\n",
    "    new1,T1 = process_review_CountVectorizer(train_data_review, ngram_range)\n",
    "    new2,T2 = process_review_CountVectorizer(test_data_review, ngram_range)\n",
    "    # 得到词频交集\n",
    "    T = set(T1)&set(T2)\n",
    "    T = list(T)\n",
    "    new1 = new1[T]\n",
    "    \n",
    "    # Train the model with word frequency by Multinomial Naive Bayes, and get the Probability for each reasult as features\n",
    "    nb1 = MultinomialNB()\n",
    "    nb1.fit(new1, train_y)\n",
    "    a = nb1.predict_proba(new1)\n",
    "    dic ={\n",
    "    '0': a[:,0],\n",
    "    '1': a[:,1],\n",
    "    '2': a[:,2]\n",
    "    }\n",
    "    new = pd.DataFrame(dic)\n",
    "    new.index.name ='merge_index_unique'\n",
    "    train_data.index.name ='merge_index_unique'\n",
    "    train_data_new = pd.merge(train_data.reset_index(),new.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    new2 = new2[T]\n",
    "\n",
    "    nb2 = MultinomialNB()\n",
    "    nb2.fit(new2, test_y)\n",
    "    a = nb2.predict_proba(new2)\n",
    "    dic ={\n",
    "    '0': a[:,0],\n",
    "    '1': a[:,1],\n",
    "    '2': a[:,2]\n",
    "    }\n",
    "    new = pd.DataFrame(dic)\n",
    "    new.index.name ='merge_index_unique'\n",
    "    test_data.index.name ='merge_index_unique'\n",
    "    test_data_new = pd.merge(test_data.reset_index(),new.reset_index(),on='merge_index_unique')\n",
    "\n",
    "    train_x = train_data_new.drop(['category','review'], axis=1)\n",
    "    test_x = test_data_new.drop(['category','review'], axis=1)\n",
    "\n",
    "    train_x = (train_x-train_x.min())/(train_x.max()-train_x.min())\n",
    "    test_x = (test_x-test_x.min())/(test_x.max()-test_x.min())\n",
    "\n",
    "    train_x = train_x.to_numpy()\n",
    "    test_x = test_x.to_numpy()\n",
    "    nb_new = GaussianNB()\n",
    "    nb_new.fit(train_x, train_y)\n",
    "    print(\"The score is :\")\n",
    "    print(nb_new.score(test_x,test_y))\n",
    "    print()\n",
    "  \n",
    "type4(train_data,train_data_review,test_data,test_data_review,(5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64893e82-e671-42f5-9bf8-4672732f4516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
